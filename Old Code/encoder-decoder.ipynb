{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choice\n",
    "import urllib.request  # the lib that handles the url stuff\n",
    "import time\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/gracexwho/drug-drug-interactions/master/ChCh-Miner_durgbank-chem-chem.tsv\"\n",
    "url_data = urllib.request.urlopen(url) \n",
    "\n",
    "G = nx.read_edgelist(url_data)\n",
    "\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "\n",
    "\n",
    "# Create an experiment\n",
    "experiment = Experiment(api_key=\"yeThLw8MLFuaMF3cVW1b9IsIt\",\n",
    "                        project_name=\"general\", workspace=\"gracexwho\")\n",
    "\n",
    "# Report any information you need by:\n",
    "\n",
    "################# CONTROL ##################\n",
    "\n",
    "hyper_params = {\"learning_rate\": 0.1, \"epochs\": 20}\n",
    "experiment.log_parameters(hyper_params)\n",
    "\n",
    "num_train = 200\n",
    "num_val = 50\n",
    "\n",
    "################# CONTROL ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(T):\n",
    "    JC = {}\n",
    "    listNodes = list(T.nodes())\n",
    "    for u in listNodes:\n",
    "        for v in listNodes:\n",
    "            common = len(list(nx.common_neighbors(T, u, v)))\n",
    "            union = T.degree[u] + T.degree[v] - common\n",
    "            if union == 0:\n",
    "                JC[(u,v)] = 0\n",
    "            else:\n",
    "                JC[(u,v)] = (common/union)\n",
    "    \n",
    "    return JC\n",
    "\n",
    "\n",
    "\n",
    "def Common_Neighbours(T):\n",
    "    CN = {}\n",
    "    listNodes = list(T.nodes())\n",
    "    for u in listNodes:\n",
    "        for v in listNodes:\n",
    "            CN[(u,v)] = len(list(nx.common_neighbors(T, u, v)))\n",
    "    return CN\n",
    "\n",
    "\n",
    "def Adamic_Adar(T):\n",
    "    AA = {}\n",
    "    listNodes = list(T.nodes())\n",
    "    for u in listNodes:\n",
    "        for v in listNodes:\n",
    "            common = nx.common_neighbors(T, u, v)\n",
    "            AA[(u,v)] = sum(1/np.log(G.degree[common]))\n",
    "    return AA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairs = [(id1, id2) â€¦]\n",
    "\n",
    "-Define ID map (pre-processing step)\n",
    "-Before training loop, do\n",
    "\n",
    "Pairs = (get_id(pair[0]) , get_id(pair[1] for pair in pairs)\n",
    "\n",
    "Id_map = {}\n",
    "Def get_id(id_string):\n",
    "\tIf id_string is in id_map:\n",
    "\t\tReturn id_map(id_string)\n",
    "\tElse:\n",
    "\t\tId = len(id_map)\n",
    "\t\tid_map(id_string) = Id\n",
    "\t\tReturn Id\n",
    "\n",
    "\n",
    "For epoch in range(num_epochs):\n",
    "Shuffle(pairs)\n",
    "Index = 0\n",
    "Batch_size = 64\n",
    "While index+batch_size < length(pairs):\n",
    "\tBatch = pairs[index:index+batch_size]      // index[min(index+batch_size, len(pairs))\n",
    "\tIndex += batch_size\n",
    "\n",
    "Def process_batch(batch):\n",
    "\tLeft_ids = LongTensor([pair[0] for pair in batch])\n",
    "\tRight_ids = LongTensor([pair[1] for pair in batch])\n",
    "\tNeg_ids = LongTensor([np.randint(0, maxnodeid) for _ in batch)\n",
    "\n",
    "\tLeft_embeds = embedding(left_ids)\n",
    "\t tensor batch size x embedding dimension\n",
    "\tRight_embeds =\n",
    "\tNeg_embeds =...\n",
    "\n",
    "\tPos_score = left_embed x right_embed then summed across axis=0\n",
    "\tNeg_score = left_embed x neg_embed\n",
    "\n",
    "\tLoss = get_loss(pos_score, neg_score)\n",
    "\n",
    "-one-hot encoding\n",
    "-random walk permutation\n",
    "\t-center node and then do pairing two left and two right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n",
      "torch.Size([1, 50])\n",
      "--- 8.0 minutes ---\n"
     ]
    }
   ],
   "source": [
    "# Get training set validation set\n",
    "\n",
    "# what if you just test on edges: instead of training on all node x node combos, just train on edges\n",
    "# for u in training_nodes, for v in list(G.adj[u])\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "graph_nodes = list(G.nodes())\n",
    "training_nodes = torch.zeros([1, num_train], dtype=torch.float)\n",
    "training_nodes = training_nodes\n",
    "validation_nodes = torch.zeros([1, num_val], dtype=torch.float)\n",
    "node_dict = {}\n",
    "# encode Nodes as numbers\n",
    "\n",
    "index = 0\n",
    "for node in list(G.nodes()):\n",
    "    node_dict[index] = node\n",
    "    index += 1\n",
    "\n",
    "    \n",
    "for i in range(num_train):\n",
    "    training_nodes[0][i] = choice(range(len(node_dict)))\n",
    "    if i < num_val:\n",
    "        validation_nodes[0][i] = choice(range(len(node_dict)))\n",
    "\n",
    "y_true = []\n",
    "for x in validation_nodes[0]:\n",
    "    node_x = node_dict[x.item()]\n",
    "    for y in validation_nodes[0]:\n",
    "        node_y = node_dict[y.item()]\n",
    "        if (node_x, node_y) in list(G.edges()):\n",
    "            y_true.append(1)\n",
    "        else:\n",
    "            y_true.append(0)\n",
    "        \n",
    "\n",
    "print(training_nodes.shape)\n",
    "print(validation_nodes.shape)\n",
    "\n",
    "\n",
    "similarity = Jaccard(G)\n",
    "\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)//60))\n",
    "\n",
    "# 11 min for 500 nodes in training_nodes, 100 in val\n",
    "# 8 min for 200 nodes in training, 50 in val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating the mapping\n",
    "# map nodes -> R^d\n",
    "# decoder in training\n",
    "# lesson 7: Encoders-Decoders\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  # should return VECTORS for each node\n",
    "    def __init__(self, num_train):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        #self.dropout = nn.Dropout(p=0.2)\n",
    "        # one layer, return embeds(node_ids) which is a long tensor\n",
    "        #learnrate might be too big if doesn't decrease\n",
    "        self.embed = nn.Embedding(num_train, 256)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        return x\n",
    "\n",
    "# embeds.weight = embeds.weight/np.sqrt(mbed_dim)\n",
    "  # Loss function can't be a float, it should be a tensor\n",
    "  # and also DON'T unwrap a tensor at any point, that gets rid of grad\n",
    "  # Keep it in tensor operators: maybe change node_dict into a tensor\n",
    "\n",
    "    \n",
    "def LossFunction(nodes, similarity):\n",
    "  # decoder represents ALREADY DECODED results\n",
    "  # similarity represents evaluated similarities\n",
    "  # decode is a tuple (u,v) of integers\n",
    "    u, v = nodes\n",
    "    a = model(torch.cuda.LongTensor([[u]]))            #input to a model on CUDA should be CUDA tensors!\n",
    "    b = model(torch.cuda.LongTensor([[v]]))\n",
    "  \n",
    "    decoded = Decoder(a,b)\n",
    "    sim = torch.tensor(similarity[(node_dict[u.item()], node_dict[v.item()])])\n",
    "    loss = decoded - sim       ##\n",
    "    loss = torch.pow(loss, 2)\n",
    "    return loss\n",
    "  \n",
    "# calculate inner product\n",
    "def Decoder(v1, v2):\n",
    "    v1 = torch.squeeze(v1)\n",
    "    v2 = torch.squeeze(v2)\n",
    "    return torch.dot(v1, v2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 189.76642837799488\n",
      "The AUC score is:  0.4356897959183673\n",
      "AUC for JC index is:  0.5\n",
      "Saving model...\n",
      "Training loss: 25.19827439197786\n",
      "The AUC score is:  0.43366530612244897\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 8.696022829730163\n",
      "The AUC score is:  0.43343673469387756\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 3.4490601763001845\n",
      "The AUC score is:  0.43407346938775504\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 1.5137561598994025\n",
      "The AUC score is:  0.4341877551020409\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.7385860222791126\n",
      "The AUC score is:  0.43405714285714286\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.40077482316904467\n",
      "The AUC score is:  0.4339591836734694\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.23992620998484948\n",
      "The AUC score is:  0.4339265306122449\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.15649955039700528\n",
      "The AUC score is:  0.43386122448979597\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.10963670454440388\n",
      "The AUC score is:  0.43397551020408165\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.08135105182402887\n",
      "The AUC score is:  0.43392653061224495\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.06315982379016231\n",
      "The AUC score is:  0.43387755102040815\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.050796833751459824\n",
      "The AUC score is:  0.43387755102040815\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.041988062397426126\n",
      "The AUC score is:  0.43377959183673476\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.035456168621218476\n",
      "The AUC score is:  0.4338448979591837\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.03044878076576933\n",
      "The AUC score is:  0.43382857142857134\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.026503241186935575\n",
      "The AUC score is:  0.4338285714285714\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.023323224054716117\n",
      "The AUC score is:  0.43382857142857145\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.02071218616763262\n",
      "The AUC score is:  0.4338612244897959\n",
      "AUC for JC index is:  0.5\n",
      "Training loss: 0.01853493543912457\n",
      "The AUC score is:  0.4338612244897959\n",
      "AUC for JC index is:  0.5\n",
      "tensor([[-0.0185, -0.1064,  0.0004,  ...,  0.0299,  0.1015,  0.0058],\n",
      "        [ 0.0684, -0.0631,  0.0223,  ...,  0.0250,  0.0115, -0.0489],\n",
      "        [ 0.0071,  0.0903,  0.0428,  ...,  0.0929, -0.0372, -0.1392],\n",
      "        ...,\n",
      "        [ 0.0472,  0.0323, -0.0183,  ...,  0.0540,  0.0328, -0.0465],\n",
      "        [ 0.0114, -0.0986, -0.0666,  ..., -0.0570, -0.0369,  0.0226],\n",
      "        [-0.0840,  0.0637, -0.0274,  ..., -0.0991,  0.0356,  0.0360]],\n",
      "       device='cuda:0')\n",
      "--- 33.0 minutes ---\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = Encoder(G.number_of_nodes())\n",
    "model.embed.weight.data = (model.embed.weight.data/np.sqrt(256))\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=hyper_params['learning_rate'])\n",
    "\n",
    "epochs = hyper_params['epochs']\n",
    "\n",
    " # have to somehow get vectors representing each node\n",
    " # pass in a SINGLE NODE to encoder and get back a vector\n",
    " # Comet.ml\n",
    "\n",
    "encoded = {}\n",
    "difference = float(\"inf\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_loss = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for u in training_nodes[0]:\n",
    "        for v in training_nodes[0]:\n",
    "            optimizer.zero_grad()\n",
    "            train_loss = LossFunction((u, v), similarity)\n",
    "            running_loss += train_loss.item()\n",
    "            train_loss.backward()        #retain_graph=True\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "\n",
    "    print(f\"Training loss: {running_loss}\")\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    y_score = []\n",
    "    index = []\n",
    "    \n",
    "    for x in validation_nodes[0]:\n",
    "        for y in validation_nodes[0]:\n",
    "            a = model(torch.cuda.LongTensor([[x]])) \n",
    "            b = model(torch.cuda.LongTensor([[y]]))  \n",
    "            result = Decoder(a, b)\n",
    "            result = result.cpu()\n",
    "            result = result.detach().numpy()\n",
    "            y_score.append(result)\n",
    "            index.append(similarity[(node_dict[u.item()], node_dict[v.item()])])\n",
    "            \n",
    "    new_auc = roc_auc_score(y_true, y_score)\n",
    "    print(\"The AUC score is: \", new_auc)\n",
    "    index_auc = roc_auc_score(y_true, index)\n",
    "    print(\"AUC for JC index is: \", index_auc)\n",
    "    \n",
    "    if (abs(new_auc - index_auc) < difference):\n",
    "        print(\"Saving model...\")\n",
    "        torch.save(model.state_dict(), 'encoder-decoder.pth')\n",
    "    difference = abs(new_auc - index_auc)\n",
    "    \n",
    "    \n",
    "\n",
    "print(model.embed.weight.data)\n",
    "\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)//60))\n",
    "\n",
    "#  for 500 training, 100 val\n",
    "# for 200 training, 50 val --- 33.0 minutes ---\n",
    "#Training loss: 0.01853493543912457\n",
    "#The AUC score is:  0.4338612244897959\n",
    "#AUC for JC index is:  0.5\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 6-3-2019\n",
    "\n",
    "#### Trends\n",
    "##### The AUC score doesn't really change, and it approaches the JC score\n",
    "\n",
    "Training loss: 206.67951894763414\n",
    "The AUC score is:  0.47640592225131295\n",
    "AUC for JC index is:  0.5\n",
    "Saving model...\n",
    "Training loss: 29.62321179928426\n",
    "The AUC score is:  0.47848820296112565\n",
    "AUC for JC index is:  0.5\n",
    "Saving model...\n",
    "Training loss: 10.369048358812345\n",
    "The AUC score is:  0.47847799570274424\n",
    "AUC for JC index is:  0.5\n",
    "Saving model...\n",
    "Training loss: 4.126018026217311\n",
    "The AUC score is:  0.4784882029611256\n",
    "AUC for JC index is:  0.5\n",
    "Saving model...\n",
    "Training loss: 1.8465247948907366\n",
    "The AUC score is:  0.47844737392759984\n",
    "AUC for JC index is:  0.5\n",
    "Saving model...\n",
    "Training loss: 0.9538654205311691\n",
    "The AUC score is:  0.4785800682865586\n",
    "AUC for JC index is:  0.5\n",
    "Saving model...\n",
    "Training loss: 0.5615920932513642\n",
    "The AUC score is:  0.4786515190952285\n",
    "AUC for JC index is:  0.5\n",
    "Saving model...\n",
    "Training loss: 0.3670247585797502\n",
    "The AUC score is:  0.47865151909522863\n",
    "AUC for JC index is:  0.5\n",
    "Saving model...\n",
    "Training loss: 0.2596153598021152\n",
    "The AUC score is:  0.47862089732008434\n",
    "AUC for JC index is:  0.5\n",
    "Saving model...\n",
    "Training loss: 0.1948714980473346\n",
    "The AUC score is:  0.4785494465114143\n",
    "AUC for JC index is:  0.5\n",
    "Saving model...\n",
    "tensor([[ 0.0036,  0.0317, -0.0416,  ..., -0.1481,  0.0023, -0.0238],\n",
    "        [ 0.0305, -0.0064, -0.0720,  ...,  0.0456, -0.0369, -0.1447],\n",
    "        [ 0.0626, -0.0218,  0.0999,  ..., -0.0788,  0.0204, -0.0226],\n",
    "        ...,\n",
    "        [ 0.0026,  0.0091, -0.0706,  ...,  0.0365, -0.0633,  0.1414],\n",
    "        [ 0.0566,  0.0866,  0.0814,  ...,  0.0723,  0.0770,  0.0155],\n",
    "        [-0.0343, -0.0056, -0.0170,  ...,  0.0677, -0.0670,  0.1061]],\n",
    "       device='cuda:0')\n",
    "--- 20.0 minutes ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
