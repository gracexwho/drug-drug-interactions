{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1514\n",
      "48514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/gracexwho/node2vec/1339ef6c37c343cf89d1772301c5d244\n",
      "COMET INFO:   Metrics:\n",
      "COMET INFO:                          loss: 1.386304259300232\n",
      "COMET INFO:         sys.gpu.0.free_memory: 4216324096\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization: 0\n",
      "COMET INFO:        sys.gpu.0.total_memory: 4294967296\n",
      "COMET INFO:         sys.gpu.0.used_memory: 78643200\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: old comet version (1.0.55) detected. current: 2.0.1 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/gracexwho/node2vec/a7767292f5de4cfd8e8d9fd6c9c118c6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choice\n",
    "import urllib.request  # the lib that handles the url stuff\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from itertools import *\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/gracexwho/drug-drug-interactions/master/ChCh-Miner_durgbank-chem-chem.tsv\"\n",
    "url_data = urllib.request.urlopen(url) \n",
    "\n",
    "G = nx.read_edgelist(url_data)\n",
    "\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "\n",
    "\n",
    "# Create an experiment\n",
    "experiment = Experiment(api_key=\"yeThLw8MLFuaMF3cVW1b9IsIt\",\n",
    "                        project_name=\"Node2Vec\", workspace=\"gracexwho\")\n",
    "\n",
    "# Report any information you need by:\n",
    "\n",
    "################# CONTROL ##################\n",
    "\n",
    "hyper_params = {\"learning_rate\": 0.03, \"epochs\": 20, \"num_walks\": 100, \"walk_length\": 10, \"window_size\": 3}\n",
    "experiment.log_parameters(hyper_params)\n",
    "\n",
    "num_train = 1200\n",
    "num_val = 100\n",
    "\n",
    "################# CONTROL ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random walks\n",
    "pairs = []\n",
    "\n",
    "for i in range(hyper_params['num_walks']):\n",
    "    current = choice(list(G.nodes()))\n",
    "    walk = [current]\n",
    "    y = []\n",
    "    \n",
    "    for w in range(hyper_params['walk_length']):\n",
    "        # walk to an adjacent node\n",
    "        # error: some adjacent nodes are NOT IN the training set\n",
    "        c = list(G.adj[current])\n",
    "        current = choice(c)\n",
    "        walk.append(current)\n",
    "    \n",
    "    # take permutations as closely related within the window size\n",
    "    y = [permutations(walk[i : i+hyper_params['window_size']], 2) for i in range(len(walk)-hyper_params['window_size'])]\n",
    "    z = []\n",
    "    for l in y:\n",
    "        z.extend(list(l))\n",
    "    pairs.extend(z)\n",
    "\n",
    "# remove duplicates\n",
    "pairs = list(dict.fromkeys(pairs))\n",
    "#print(pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairs = [(id1, id2) â€¦]\n",
    "\n",
    "-Define ID map (pre-processing step)\n",
    "-Before training loop, do\n",
    "\n",
    "Pairs = (get_id(pair[0]) , get_id(pair[1] for pair in pairs)\n",
    "\n",
    "Id_map = {}\n",
    "Def get_id(id_string):\n",
    "\tIf id_string is in id_map:\n",
    "\t\tReturn id_map(id_string)\n",
    "\tElse:\n",
    "\t\tId = len(id_map)\n",
    "\t\tid_map(id_string) = Id\n",
    "\t\tReturn Id\n",
    "\n",
    "\n",
    "For epoch in range(num_epochs):\n",
    "Shuffle(pairs)\n",
    "Index = 0\n",
    "Batch_size = 64\n",
    "While index+batch_size < length(pairs):\n",
    "\tBatch = pairs[index:index+batch_size]      // index[min(index+batch_size, len(pairs))\n",
    "\tIndex += batch_size\n",
    "\n",
    "Def process_batch(batch):\n",
    "\tLeft_ids = LongTensor([pair[0] for pair in batch])\n",
    "\tRight_ids = LongTensor([pair[1] for pair in batch])\n",
    "\tNeg_ids = LongTensor([np.randint(0, maxnodeid) for _ in batch)\n",
    "\n",
    "\tLeft_embeds = embedding(left_ids)\n",
    "\t tensor batch size x embedding dimension\n",
    "\tRight_embeds =\n",
    "\tNeg_embeds =...\n",
    "\n",
    "\tPos_score = left_embed x right_embed then summed across axis=0\n",
    "\tNeg_score = left_embed x neg_embed\n",
    "\n",
    "\tLoss = get_loss(pos_score, neg_score)\n",
    "\n",
    "-one-hot encoding\n",
    "-random walk permutation\n",
    "\t-center node and then do pairing two left and two right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating the mapping\n",
    "# map nodes -> R^d\n",
    "# decoder in training\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  # should return VECTORS for each node\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        #self.dropout = nn.Dropout(p=0.2)\n",
    "        # one layer, return embeds(node_ids) which is a long tensor\n",
    "        #learnrate might be too big if doesn't decrease\n",
    "        self.embed = nn.Embedding(G.number_of_nodes(), 256)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # take the node name as input and \n",
    "        x = self.embed(x)\n",
    "        return x\n",
    "\n",
    "# embeds.weight = embeds.weight/np.sqrt(mbed_dim)\n",
    "  # Loss function can't be a float, it should be a tensor\n",
    "  # and also DON'T unwrap a tensor at any point, that gets rid of grad\n",
    "  # Keep it in tensor operators: maybe change node_dict into a tensor\n",
    "\n",
    "    \n",
    "def LossFunction(u, v, n):\n",
    "  # Now this takes in 3 MATRICES\n",
    "# Sum over node pairs: -log(sigmoid(dot prod)) - sum over n in N (log (1-sigma(dot prod zii zn)))\n",
    "# N = randomly sample\n",
    "  \n",
    "    similar = Decoder(u, v)\n",
    "    diff = Decoder(u, n)\n",
    "    \n",
    "    loss = -np.log(torch.mean(nn.Sigmoid(similar))) - np.log(torch.mean(1 - nn.Sigmoid(diff)))  \n",
    "    \n",
    "    return loss\n",
    "  \n",
    "    \n",
    "# calculate inner product between 2 matrices\n",
    "def Decoder(a, b):\n",
    "    c = []\n",
    "    for row_a in a:\n",
    "        for row_b in b:\n",
    "            c.append(torch.dot(row_a, row_b))\n",
    "    \n",
    "    return torch.cuda.FloatTensor(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(70.7134, grad_fn=<AddBackward0>)\n",
      "tensor(70.7134, grad_fn=<AddBackward0>)\n",
      "tensor(70.7128, grad_fn=<AddBackward0>)\n",
      "tensor(70.7138, grad_fn=<AddBackward0>)\n",
      "tensor(70.7137, grad_fn=<AddBackward0>)\n",
      "tensor(70.7144, grad_fn=<AddBackward0>)\n",
      "tensor(70.7134, grad_fn=<AddBackward0>)\n",
      "tensor(70.7131, grad_fn=<AddBackward0>)\n",
      "tensor(70.7136, grad_fn=<AddBackward0>)\n",
      "tensor(70.7129, grad_fn=<AddBackward0>)\n",
      "tensor(70.7136, grad_fn=<AddBackward0>)\n",
      "tensor(70.7132, grad_fn=<AddBackward0>)\n",
      "tensor(70.7138, grad_fn=<AddBackward0>)\n",
      "tensor(70.7126, grad_fn=<AddBackward0>)\n",
      "tensor(70.7133, grad_fn=<AddBackward0>)\n",
      "tensor(70.7136, grad_fn=<AddBackward0>)\n",
      "tensor(70.7128, grad_fn=<AddBackward0>)\n",
      "tensor(70.7127, grad_fn=<AddBackward0>)\n",
      "tensor(70.7132, grad_fn=<AddBackward0>)\n",
      "tensor(70.7132, grad_fn=<AddBackward0>)\n",
      "tensor(70.7133, grad_fn=<AddBackward0>)\n",
      "tensor(70.7130, grad_fn=<AddBackward0>)\n",
      "tensor(70.7130, grad_fn=<AddBackward0>)\n",
      "tensor(70.7120, grad_fn=<AddBackward0>)\n",
      "tensor(70.7134, grad_fn=<AddBackward0>)\n",
      "tensor(70.7133, grad_fn=<AddBackward0>)\n",
      "tensor(70.7132, grad_fn=<AddBackward0>)\n",
      "tensor(70.7132, grad_fn=<AddBackward0>)\n",
      "tensor(70.7131, grad_fn=<AddBackward0>)\n",
      "tensor(70.7124, grad_fn=<AddBackward0>)\n",
      "tensor(70.7136, grad_fn=<AddBackward0>)\n",
      "tensor(70.7124, grad_fn=<AddBackward0>)\n",
      "tensor(70.7127, grad_fn=<AddBackward0>)\n",
      "tensor(70.7133, grad_fn=<AddBackward0>)\n",
      "tensor(70.7136, grad_fn=<AddBackward0>)\n",
      "tensor(70.7133, grad_fn=<AddBackward0>)\n",
      "tensor(70.7125, grad_fn=<AddBackward0>)\n",
      "tensor(70.7129, grad_fn=<AddBackward0>)\n",
      "tensor(70.7130, grad_fn=<AddBackward0>)\n",
      "tensor(70.7130, grad_fn=<AddBackward0>)\n",
      "tensor(70.7124, grad_fn=<AddBackward0>)\n",
      "tensor(70.7133, grad_fn=<AddBackward0>)\n",
      "tensor(70.7134, grad_fn=<AddBackward0>)\n",
      "tensor(70.7132, grad_fn=<AddBackward0>)\n",
      "tensor(70.7140, grad_fn=<AddBackward0>)\n",
      "tensor(70.7136, grad_fn=<AddBackward0>)\n",
      "tensor(70.7122, grad_fn=<AddBackward0>)\n",
      "tensor(70.7133, grad_fn=<AddBackward0>)\n",
      "tensor(70.7127, grad_fn=<AddBackward0>)\n",
      "tensor(70.7132, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "id_map = {}\n",
    "\n",
    "\n",
    "def get_id(id_string):\n",
    "    if id_string in id_map.keys():\n",
    "        return id_map[id_string]\n",
    "    else:\n",
    "        ID = len(id_map)\n",
    "        id_map[id_string] = ID\n",
    "        return ID\n",
    "    \n",
    "        \n",
    "def process_batch(batch):\n",
    "    left_ids = torch.LongTensor([pair[0] for pair in batch])\n",
    "    right_ids = torch.LongTensor([pair[1] for pair in batch])\n",
    "    neg_ids = torch.LongTensor([np.random.randint(0, G.number_of_nodes()) for _ in batch])\n",
    "    \n",
    "    #print(left_ids)\n",
    "    left_embeds = model(left_ids)\n",
    "    right_embeds = model(right_ids)\n",
    "    neg_embeds = model(neg_ids)\n",
    "    \n",
    "    pos_score = torch.mm(torch.t(left_embeds), right_embeds)\n",
    "    neg_score = torch.mm(torch.t(left_embeds), neg_embeds)\n",
    "    \n",
    "    loss = get_loss(pos_score, neg_score)\n",
    "    return loss\n",
    "    \n",
    "                          \n",
    "def get_loss(pos, neg):\n",
    "    m = nn.Sigmoid()\n",
    "    loss = -torch.mean(torch.log(m(pos))) - torch.mean(torch.log(1 - m(neg)))\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "model = Encoder()\n",
    "model.embed.weight.data = (model.embed.weight.data/np.sqrt(256))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=hyper_params['learning_rate'])\n",
    "\n",
    "epochs = hyper_params['epochs']\n",
    "\n",
    "pairs = [(get_id(pair[0]) , get_id(pair[1])) for pair in pairs]\n",
    "\n",
    "for e in range(50):\n",
    "    random.shuffle(pairs)\n",
    "    train_loss = 0\n",
    "    batch_size = 64\n",
    "    batch = []\n",
    "    index=0\n",
    "    \n",
    "    while index+batch_size < len(pairs):\n",
    "        batch = pairs[index:min(index+batch_size, len(pairs))]\n",
    "        index += batch_size\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = process_batch(batch)\n",
    "        #print(batch)\n",
    "        train_loss += loss\n",
    "        loss.backward()        #retain_graph=True\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(train_loss)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1200])\n",
      "torch.Size([1, 100])\n",
      "--- 0.0 minutes ---\n"
     ]
    }
   ],
   "source": [
    "# Generate Training/Validation Set\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# generate the pairs FIRST THEN split the pairs into validation set and training set\n",
    "\n",
    "training_nodes = torch.zeros([1, num_train], dtype=torch.float)\n",
    "validation_nodes = torch.zeros([1, num_val], dtype=torch.float)\n",
    "node_dict = {}\n",
    "# encode Nodes as numbers\n",
    "\n",
    "index = 0\n",
    "for node in list(G.nodes()):\n",
    "    node_dict[index] = node\n",
    "    index += 1\n",
    "\n",
    "training = []\n",
    "validation = []\n",
    "\n",
    "graph_nodes = list(range(len(node_dict)))\n",
    "\n",
    "# SHOULD SEPARATE VALIDATION NODES FROM TRAINING NODES\n",
    "# choose from a list of nodes\n",
    "    \n",
    "for i in range(num_train):\n",
    "    c = choice(graph_nodes)\n",
    "    training_nodes[0][i] = c\n",
    "    graph_nodes.remove(c)\n",
    "    training.append(node_dict[c])\n",
    "    \n",
    "    if i < num_val:\n",
    "        validation_nodes[0][i] = choice(graph_nodes)\n",
    "        validation.append(node_dict[c])\n",
    "\n",
    "y_true = []\n",
    "#for x in validation_nodes[0]:\n",
    "#    node_x = node_dict[x.item()]\n",
    "#    for y in validation_nodes[0]:\n",
    "#        node_y = node_dict[y.item()]\n",
    "#        if (node_x, node_y) in list(G.edges()):\n",
    "#            y_true.append(1)\n",
    "#        else:\n",
    "#            y_true.append(0)\n",
    "        \n",
    "\n",
    "print(training_nodes.shape)\n",
    "print(validation_nodes.shape)\n",
    "\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)//60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#pairs_train = pairs[0:num_train]\n",
    "#pairs_val = pairs[num_train:num_train+num_val]\n",
    "\n",
    "#print(len(pairs_train))\n",
    "#print(len(pairs_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1395, -1.7461, -0.9168,  1.2227],\n",
      "        [ 0.8206,  0.7132,  0.1555,  0.7413],\n",
      "        [ 0.8960,  0.8743, -0.6854, -0.6067],\n",
      "        [-1.4658, -0.9448, -0.3169,  0.8085]])\n",
      "tensor([[-0.0531, -1.3703,  1.6524,  1.8320],\n",
      "        [-0.4788,  1.2953,  0.4166, -0.7193],\n",
      "        [-0.4933, -2.2002, -1.4886,  0.2413],\n",
      "        [ 0.6649,  0.5782, -0.9959,  1.1086]])\n",
      "tensor([ 3.1104, -3.5900,  5.4327,  1.3517,  0.5941,  0.0625, -2.0264,  1.6249,\n",
      "        -3.4897,  0.8544, -1.4916,  1.1113,  2.3300, -1.2357,  3.4686, -0.3089],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "b = torch.randn(4, 4)\n",
    "\n",
    "c = Decoder(a, b)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = Encoder(G.number_of_nodes())\n",
    "model.embed.weight.data = (model.embed.weight.data/np.sqrt(256))\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=hyper_params['learning_rate'])\n",
    "\n",
    "epochs = hyper_params['epochs']\n",
    "\n",
    " # have to somehow get vectors representing each node\n",
    " # pass in a SINGLE NODE to encoder and get back a vector\n",
    " # Comet.ml\n",
    "\n",
    "encoded = {}\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_loss = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    # generate a matrix for say 10 (u,v) pairs in Pairs and then use torch tensor operations to calculate loss\n",
    "    # you're generating a matrix of NODE EMBEDDINGS for 10 u then 10 v\n",
    "    \n",
    "    u_matrix = torch.zeros(1, 256)\n",
    "    v_matrix = torch.zeros(1, 256)\n",
    "    n_matrix = torch.zeros(1, 256)\n",
    "    \n",
    "    for loop in range(100):\n",
    "    \n",
    "        for index in range(64):\n",
    "        # should I pop the pair chosen to make sure it isn't chosen again for now?\n",
    "            (u,v) = choice(pairs)\n",
    "            n = choice(training_nodes[0])\n",
    "            node_u = model(torch.cuda.LongTensor([[u]])) \n",
    "            node_v = model(torch.cuda.LongTensor([[v]]))\n",
    "            node_n = model(torch.cuda.LongTensor([[n]]))\n",
    "            torch.cat((u_matrix, node_u), dim=0)\n",
    "            torch.cat((v_matrix, node_v), dim=0)\n",
    "            torch.cat((v_matrix, node_n), dim=0)\n",
    "    \n",
    "        u_matrix = u_matrix[1:]\n",
    "        v_matrix = v_matrix[1:]\n",
    "        n_matrix = n_matrix[1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss = LossFunction(u_matrix, v_matrix, n_matrix)\n",
    "        running_loss += train_loss.item()\n",
    "        train_loss.backward()        #retain_graph=True\n",
    "        optimizer.step()\n",
    "            \n",
    "            \n",
    "\n",
    "    print(f\"Training loss: {running_loss}\")\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    y_score = []\n",
    "    index = []\n",
    "    \n",
    "    for x in validation_nodes[0]:\n",
    "        for y in validation_nodes[0]:\n",
    "            a = model(torch.cuda.LongTensor([[x]])) \n",
    "            b = model(torch.cuda.LongTensor([[y]]))  \n",
    "            result = Decoder(a, b)\n",
    "            result = result.cpu()\n",
    "            result = result.detach().numpy()\n",
    "            y_score.append(result)\n",
    "            index.append(similarity[(node_dict[u.item()], node_dict[v.item()])])\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "print(model.embed.weight.data)\n",
    "\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)//60))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x0000024724E15AE8>\n"
     ]
    }
   ],
   "source": [
    "#x = [2, 3, 4, 5, 6]\n",
    "#y = map(lambda v : v * 5, x)\n",
    "b = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "b = lambda c: [a for b in c for a in b]\n",
    "print(b)\n",
    "\n",
    "#print(list(y))\n",
    "#a = ['A', 'B', 'C', 'D']\n",
    "#print(a[1 : 3])\n",
    "#print(list(permutations(a, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-1c15f40f38fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "print(choice(a - 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejected Code from node2vec-link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "    def forward(self, x):\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "\n",
    "#embedding = nn.Embedding.from_pretrained(weight)\n",
    "\n",
    "\n",
    "# index out of range: maybe you should get_id for all of the nodes originally\n",
    "\n",
    "#left = [pair[0] for pair in train_edges]\n",
    "#right = [pair[1] for pair in train_edges]\n",
    "#left = [get_id(l) for l in left]\n",
    "#right = [get_id(r) for r in right]\n",
    "    \n",
    "#left_ids = torch.cat([weight[ids] for ids in left], -1)\n",
    "#right_ids = torch.cat([weight[ids] for ids in right], -1)\n",
    "    \n",
    "#dot_prod = torch.dot(left_ids, right_ids)\n",
    "#element_mul = left_ids * right_ids\n",
    "#print(dot_prod)\n",
    "#print(element_mul.shape)\n",
    "\n",
    "# need to implement negatives as well\n",
    "# change y_label and dp into TENSORS\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_edges = list(T.edges())\n",
    "val_edges = list(V.edges())\n",
    "\n",
    "y_label = torch.ones((len(train_edges), 1))\n",
    "neg = torch.zeros((num_neg, 1))\n",
    "y_label = torch.cat((y_label, neg))\n",
    "y_label = torch.squeeze(y_label)\n",
    "\n",
    "\n",
    "negs = random.sample(list(nx.non_edges(T)), num_neg)\n",
    "train_edges = train_edges + negs\n",
    "\n",
    "\n",
    "link_model = LogisticRegression()\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer_link = torch.optim.SGD(link_model.parameters(), lr = 0.1) \n",
    "weight = model.embed.weight.data\n",
    "link_model.to('cuda')\n",
    "\n",
    "\n",
    "left = [pair[0] for pair in train_edges]\n",
    "right = [pair[1] for pair in train_edges]\n",
    "left = [get_id(l) for l in left]\n",
    "right = [get_id(r) for r in right]\n",
    "    \n",
    "    \n",
    "left_ids = [weight[ids] for ids in left]\n",
    "lf = torch.stack(left_ids)\n",
    "right_ids = [weight[ids] for ids in right]\n",
    "rg = torch.stack(right_ids)\n",
    "    \n",
    "\n",
    "    \n",
    "dot_prod = torch.bmm(lf.view(num_train, 1, 256), rg.view(num_train, 256, 1)) \n",
    "dot_prod = torch.squeeze(dot_prod)\n",
    "dot_prod = dot_prod.cuda()\n",
    "    \n",
    "dotproduct = dot_prod.cpu()\n",
    "dotproduct = dotproduct.numpy()\n",
    "y_label_roc = y_label.numpy()\n",
    "    \n",
    "element_mul = lf * rg\n",
    "    \n",
    "for e in range(20):\n",
    "\n",
    "    score = roc_auc_score(y_label_roc, dotproduct)\n",
    "    print(score)\n",
    "    pred_y = link_model.linear.weight.data.cpu().numpy()\n",
    "    score2 = roc_auc_score(y_label_roc, pred_y()\n",
    "    print(score2)\n",
    "           \n",
    "    y_pred = link_model(dot_prod)\n",
    "    loss = criterion(y_pred, y_label)\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    print(loss)\n",
    "        \n",
    "        \n",
    "    #dp.append(dot_prod)\n",
    "        \n",
    "    #dp = torch.t(torch.FloatTensor(dp))\n",
    "    #print(dp.shape)\n",
    "    \n",
    "\n",
    "# or instead do AUC_score somehow...just generate a whole list of y_pred and then do AUC \n",
    "    \n",
    "\n",
    "        \n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)//60))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# unbatched ################\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "dp = []\n",
    "link_model = LogisticRegression()\n",
    "alpha = 0.9\n",
    "\n",
    "for e in range(20):\n",
    "\n",
    "    link_loss = 0\n",
    "    \n",
    "    for u, v in train_edges:\n",
    "        y_label = torch.FloatTensor([0])\n",
    "        u_embed = weight[get_id(u)]\n",
    "        v_embed = weight[get_id(v)]\n",
    "        \n",
    "        dot_prod = torch.dot(u_embed, v_embed)\n",
    "        element_mul = u_embed * v_embed\n",
    "        dp.append(dot_prod.item())\n",
    "        \n",
    "        dot_prod = torch.FloatTensor([dot_prod.item()])\n",
    "\n",
    "        \n",
    "        if (u,v) in list(G.edges()):\n",
    "            y_label = torch.FloatTensor([1])\n",
    "            \n",
    "        y_pred = link_model(dot_prod)\n",
    "        loss = criterion(y_pred, y_label)\n",
    "        link_loss = alpha * link_loss + (1-alpha)*loss\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "    \n",
    "    print(link_loss/(len(train_edges)+num_neg))\n",
    "    \n",
    "score = roc_auc_score(y_label_roc, np.array(dp))\n",
    "print(score)\n",
    "pred_y = link_model.linear.weight.data.numpy()\n",
    "score2 = roc_auc_score(y_label_roc, pred_y)\n",
    "print(score2)\n",
    "\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)//60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "test1 = [1, 2, 3]\n",
    "test2 = [3, 4, 5]\n",
    "test3 = (test1, test2)\n",
    "\n",
    "for t1 in test3:\n",
    "    print(t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
