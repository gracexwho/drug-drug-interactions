{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1514\n",
      "48514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: old comet version (2.0.2) detected. current: 2.0.12 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/gracexwho/node2vec/7cdc075592c84e59af2a36fbd129868d\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choice\n",
    "import urllib.request  # the lib that handles the url stuff\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from itertools import *\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/gracexwho/drug-drug-interactions/master/ChCh-Miner_durgbank-chem-chem.tsv\"\n",
    "url_data = urllib.request.urlopen(url) \n",
    "\n",
    "G = nx.read_edgelist(url_data)\n",
    "\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "\n",
    "\n",
    "# Create an experiment\n",
    "experiment = Experiment(api_key=\"yeThLw8MLFuaMF3cVW1b9IsIt\",\n",
    "                        project_name=\"Node2Vec\", workspace=\"gracexwho\")\n",
    "\n",
    "# Report any information you need by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################# CONTROL ##################\n",
    "\n",
    "# 0.1, 5000, 100, 5, 3\n",
    "\n",
    "hyper_params = {\"learning_rate\": 0.05, \"epochs\": 10000, \"num_walks\": 1000, \"walk_length\": 5, \"window_size\": 3}\n",
    "experiment.log_parameters(hyper_params)\n",
    "\n",
    "\n",
    "################# CONTROL ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Generate random walks\n",
    "pairs = []\n",
    "\n",
    "for i in range(hyper_params['num_walks']):\n",
    "    current = choice(list(G.nodes()))\n",
    "    walk = [current]\n",
    "    y = []\n",
    "    \n",
    "    for w in range(hyper_params['walk_length']):\n",
    "        # walk to an adjacent node\n",
    "        # error: some adjacent nodes are NOT IN the training set\n",
    "        c = list(G.adj[current])\n",
    "        current = choice(c)\n",
    "        walk.append(current)\n",
    "    \n",
    "    # take permutations as closely related within the window size\n",
    "    y = [permutations(walk[i : i+hyper_params['window_size']], 2) for i in range(len(walk)-hyper_params['window_size'])]\n",
    "    z = []\n",
    "    for l in y:\n",
    "        z.extend(list(l))\n",
    "    pairs.extend(z)\n",
    "\n",
    "# remove duplicates\n",
    "pairs = list(dict.fromkeys(pairs))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  # should return VECTORS for each node\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        #self.dropout = nn.Dropout(p=0.2)\n",
    "        # one layer, return embeds(node_ids) which is a long tensor\n",
    "        #learnrate might be too big if doesn't decrease\n",
    "        self.embed = nn.Embedding(G.number_of_nodes(), 64)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # take the node name as input and \n",
    "        x = self.embed(x)\n",
    "        return x\n",
    "\n",
    "# embeds.weight = embeds.weight/np.sqrt(mbed_dim)\n",
    "  # Loss function can't be a float, it should be a tensor\n",
    "  # and also DON'T unwrap a tensor at any point, that gets rid of grad\n",
    "  # Keep it in tensor operators: maybe change node_dict into a tensor\n",
    "\n",
    "\n",
    "def common_neighbours(T, u, v):\n",
    "    Unodes = list(T.adj[u])\n",
    "    Vnodes = list(T.adj[v])\n",
    "    matches = [x for x in Unodes if x in Vnodes]\n",
    "    return iter(matches)\n",
    "\n",
    "def jaccard_index(T, u, v):\n",
    "    common = len(list(common_neighbours(T, u, v)))\n",
    "    union = T.degree[u] + T.degree[v] - common\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (common/union)\n",
    "\n",
    "def adamic_adar(T, u, v):\n",
    "    common = common_neighbours(T, u, v)\n",
    "    total = 0\n",
    "    for c in common:\n",
    "        total = total + 1/np.log(T.degree[c])\n",
    "    return total\n",
    "\n",
    "def CN(T, edges):\n",
    "    CN = {}\n",
    "    for (u,v) in edges:\n",
    "        CN[(u,v)] = len(list(common_neighbours(T,u,v)))\n",
    "    return CN.values()\n",
    "\n",
    "\n",
    "def JC(T, edges):\n",
    "    JC = {}\n",
    "    for (u,v) in edges:\n",
    "        JC[(u,v)] = jaccard_index(T, u, v)\n",
    "    return JC.values()\n",
    "\n",
    "\n",
    "def AA(T, edges):\n",
    "    AA = {}\n",
    "    for (u,v) in edges:\n",
    "        AA[(u,v)] = adamic_adar(T, u, v)\n",
    "    return AA.values()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  tensor(1.3897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Training loss:  tensor(1.3904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Training loss:  tensor(1.3901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Training loss:  tensor(1.3899, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1f3bb5319b48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;31m#change back to loss if retain_graph error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m#print(\"loss\", loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hamilton-lab\\lib\\site-packages\\comet_ml\\monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m                     )\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[0mreturn_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;31m# Call after callbacks once we have the return value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hamilton-lab\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hamilton-lab\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "## Always \"Restart Kernel and Clear Output if you're going to train again\"\n",
    "\n",
    "\n",
    "id_map = {}\n",
    "\n",
    "\n",
    "\n",
    "def get_id(id_string):\n",
    "    if id_string in id_map.keys():\n",
    "        return id_map[id_string]\n",
    "    else:\n",
    "        ID = len(id_map)\n",
    "        id_map[id_string] = ID\n",
    "        return ID\n",
    "    \n",
    "        \n",
    "def process_batch(batch):\n",
    "    left_ids = torch.cuda.LongTensor([pair[0] for pair in batch])\n",
    "    right_ids = torch.cuda.LongTensor([pair[1] for pair in batch])\n",
    "    neg_ids = torch.cuda.LongTensor([np.random.randint(0, G.number_of_nodes()) for _ in batch])\n",
    "    \n",
    "    #print(left_ids)\n",
    "    left_embeds = model(left_ids)\n",
    "    right_embeds = model(right_ids)\n",
    "    neg_embeds = model(neg_ids)\n",
    "    \n",
    "    pos_score = torch.mm(torch.t(left_embeds), right_embeds)\n",
    "    neg_score = torch.mm(torch.t(left_embeds), neg_embeds)\n",
    "    \n",
    "    loss = get_loss(pos_score, neg_score)\n",
    "    return loss\n",
    "    \n",
    "                          \n",
    "def get_loss(pos, neg):\n",
    "    m = nn.Sigmoid()\n",
    "    loss = -torch.mean(torch.log(m(pos))) - torch.mean(torch.log(1 - m(neg)))\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "model = Encoder()\n",
    "model.embed.weight.data = (model.embed.weight.data/np.sqrt(64))\n",
    "model.to('cuda')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=hyper_params['learning_rate'])\n",
    "\n",
    "epochs = hyper_params['epochs']\n",
    "\n",
    "for n in list(list(G.nodes())):\n",
    "    get_id(n)\n",
    "\n",
    "pairs = [(get_id(pair[0]) , get_id(pair[1])) for pair in pairs]\n",
    "\n",
    "\n",
    "alpha = 0.9\n",
    "train_loss = 0\n",
    "ep = hyper_params['epochs']\n",
    "\n",
    "for e in range(ep):\n",
    "    random.shuffle(pairs)\n",
    "    train_loss = 0\n",
    "    batch_size = 64\n",
    "    batch = []\n",
    "    index=0\n",
    "    # the reason you can't use training_loss for optimizer is because train_loss isn't defined within the while loop\n",
    "    # try doing train_loss.back() OUTSIDE of while loop?\n",
    "    \n",
    "    while index+batch_size < len(pairs):\n",
    "        batch = pairs[index:min(index+batch_size, len(pairs))]\n",
    "        index += batch_size\n",
    "                \n",
    "        loss = process_batch(batch)\n",
    "        train_loss = alpha * train_loss + (1-alpha)*loss\n",
    "        \n",
    "    optimizer.zero_grad() \n",
    "    train_loss.backward()        #change back to loss if retain_graph error\n",
    "    optimizer.step()\n",
    "        #print(\"loss\", loss)\n",
    "        \n",
    "    if e % 500 == 0:\n",
    "        print(\"Training loss: \", train_loss)\n",
    "        \n",
    "\n",
    "      \n",
    "torch.save(model.state_dict(), 'node2vec.pth')\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split graph into training and validation set\n",
    "import random\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/gracexwho/drug-drug-interactions/master/ChCh-Miner_durgbank-chem-chem.tsv\"\n",
    "url_data = urllib.request.urlopen(url) \n",
    "T = nx.read_edgelist(url_data)\n",
    "V = nx.Graph()\n",
    "\n",
    "# WAIT is it because the EMBEDDINGS are based on the TRAINING GRAPH and then THAT'S REMOVED\n",
    "\n",
    "num_val = 5000\n",
    "num_neg = 5000\n",
    "\n",
    "\n",
    "val_set = random.sample(list(T.edges()), num_val)\n",
    "train_set = list(T.edges())\n",
    "train_set = [e for e in train_set if e not in val_set]\n",
    "\n",
    "#T.remove_edges_from(val_set)\n",
    "#V.add_edges_from(val_set)\n",
    "\n",
    "#T.remove_nodes_from(list(nx.isolates(T)))\n",
    "# this removes nodes that don't have any neighbors from training graph\n",
    "\n",
    "num_train = num_neg + len(train_set)\n",
    "\n",
    "\n",
    "print(\"Number of validation edges\", num_val)\n",
    "print(\"Number of training edges\", num_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################### TRAINING LOOP ######################\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(64, 1)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "train_edges = train_set\n",
    "val_edges = val_set\n",
    "\n",
    "y_label = [1]*len(train_edges)\n",
    "neg = [0]*num_neg\n",
    "y_label = y_label + neg\n",
    "y_label = np.array(y_label)\n",
    "\n",
    "\n",
    "negs = random.sample(list(nx.non_edges(T)), num_neg)\n",
    "train_edges = train_edges + negs\n",
    "\n",
    "link_model = LogisticRegression()\n",
    "criterion = torch.nn.BCELoss(reduction='mean')         # this is already binary cross entropy!\n",
    "optimizer_link = torch.optim.SGD(link_model.parameters(), lr = 0.03) \n",
    "weight = model.embed.weight.data * 10\n",
    "\n",
    "\n",
    "left = [pair[0] for pair in train_edges]\n",
    "right = [pair[1] for pair in train_edges]\n",
    "left = [get_id(l) for l in left]\n",
    "right = [get_id(r) for r in right]\n",
    "left_ids = [weight[ids] for ids in left]\n",
    "lf = torch.stack(left_ids)\n",
    "right_ids = [weight[ids] for ids in right]\n",
    "rg = torch.stack(right_ids)\n",
    "\n",
    "\n",
    "#weight = torch.FloatTensor(np.random.rand(weight.shape[0], weight.shape[1]))\n",
    "\n",
    "dot_prod = torch.bmm(lf.view(num_train, 1, 64), rg.view(num_train, 64, 1)) \n",
    "dot_prod = torch.squeeze(dot_prod)\n",
    "dot_prod = dot_prod.cpu()\n",
    "dotproduct = dot_prod.numpy()\n",
    "\n",
    "score = roc_auc_score(y_label, dotproduct)\n",
    "print(\"The auc score for dot product is: \", score)\n",
    "\n",
    "\n",
    "link_model.to('cuda')\n",
    "\n",
    "\n",
    "element_mul = (lf * rg)\n",
    "#element_mul = (lf + rg)\n",
    "element_mul = element_mul.cuda()\n",
    "y_label_model = torch.cuda.FloatTensor(y_label)\n",
    "\n",
    "print(element_mul[0])\n",
    "\n",
    "link_model.train()\n",
    "\n",
    "\n",
    "# Keep it at 5000, more is overfitting\n",
    "\n",
    "ra = 50000\n",
    "for e in range(ra):\n",
    "    optimizer_link.zero_grad() \n",
    "    y_pred = link_model(element_mul)\n",
    "    y_pred = torch.squeeze(y_pred)\n",
    "    loss = criterion(y_pred, y_label_model)\n",
    "    loss.backward() \n",
    "    optimizer_link.step() \n",
    "    auc = roc_auc_score(y_label, y_pred.cpu().detach().numpy())\n",
    "    if e % 2000 == 0:\n",
    "        print(auc)\n",
    "        print(loss)\n",
    "    \n",
    "print(\"Here's the graph index value:\")\n",
    "test = list(AA(T, train_edges))\n",
    "print(roc_auc_score(y_label, test))\n",
    "    \n",
    "torch.save(link_model.state_dict(), 'linkpred.pth')\n",
    "        \n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)//60))\n",
    "    \n",
    "\n",
    "# If you get CUDA error, just keep running again and it'll work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########### VALIDATION LOOP ##############\n",
    "y_hat = []\n",
    "y_true = [1]*len(val_edges)\n",
    "neg = [0]*num_val\n",
    "y_true = y_true + neg\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "\n",
    "negs = random.sample(list(nx.non_edges(V)), num_val)\n",
    "val_edges = val_edges + negs\n",
    "\n",
    "\n",
    "link_model.eval()\n",
    "\n",
    "for (x,y) in val_edges:\n",
    "    u = weight[get_id(x)]\n",
    "    v = weight[get_id(y)]\n",
    "    em = u * v\n",
    "    em = em.cuda()\n",
    "    pred = link_model(em)\n",
    "    y_hat.append(pred.item())\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = roc_auc_score(y_true, y_hat)\n",
    "\n",
    "jaccard = list(JC(T, val_edges))\n",
    "jaccard = roc_auc_score(y_true, jaccard)\n",
    "\n",
    "adamic = list(AA(T, val_edges))\n",
    "adamic = roc_auc_score(y_true, adamic)\n",
    "\n",
    "common = list(CN(T, val_edges))\n",
    "common = roc_auc_score(y_true, common)\n",
    "\n",
    "\n",
    "print(\"The auc score for dot product is: \", score)\n",
    "print(\"The auc score for elementwise mul: \", accuracy)\n",
    "print(\"The auc score for JC index: \", jaccard)\n",
    "print(\"The auc score for AA index: \", adamic)\n",
    "print(\"The auc score for CN index: \", common)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[fpr, tpr, thresh] = roc_curve(y_true, common)\n",
    "\n",
    "ratioMax = tpr - fpr\n",
    "\n",
    "fig1 = plt.figure()\n",
    "common_line = plt.plot(fpr, tpr)\n",
    "fig1.suptitle('AUC of Local Similarity Indices', fontsize=20)\n",
    "plt.xlabel('FPR', fontsize=18)\n",
    "plt.ylabel('TPR', fontsize=16)\n",
    "plt.axis([0, 1.0, 0, 1.0])                      # set [xmin, xmax, ymin, ymax]\n",
    "\n",
    "\n",
    "[fpr2, tpr2, thresh2] = roc_curve(y_true, jaccard)\n",
    "ratioMax = tpr2 - fpr2\n",
    "jaccard_line = plt.plot(fpr2, tpr2)\n",
    "\n",
    "\n",
    "[fpr3, tpr3, thresh3] = roc_curve(y_true, adamic)\n",
    "ratioMax = tpr3 - fpr3\n",
    "adamic_line = plt.plot(fpr3, tpr3)\n",
    "\n",
    "\n",
    "legend((common_line, jaccard_line, adamic_line), ('CN', 'JC', 'AA'))\n",
    "\n",
    "\n",
    "fig1.savefig('Node2Vec_Indices.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Score\n",
    "\n",
    "Trial 1:\n",
    "\n",
    "The auc score for dot product is:  0.5026712408650089\n",
    "\n",
    "The auc score for elementwise mul:  0.5065504\n",
    "\n",
    "The auc score for JC index:  0.5927655799999999\n",
    "\n",
    "The auc score for AA index:  0.5989542999999999\n",
    "\n",
    "The auc score for CN index:  0.59798122\n",
    "\n",
    "\n",
    "Trial 2:\n",
    "\n",
    "The auc score for dot product is:  0.5047348876223744\n",
    "\n",
    "The auc score for elementwise mul:  0.49794066000000003\n",
    "\n",
    "The auc score for JC index:  0.59610572\n",
    "\n",
    "The auc score for AA index:  0.6006679199999999\n",
    "\n",
    "The auc score for CN index:  0.60034756\n",
    "\n",
    "\n",
    "Trial 3: 5000 epochs for LR training\n",
    "\n",
    "The auc score for dot product is:  0.4985369616675093\n",
    "\n",
    "The auc score for elementwise mul:  0.50202444\n",
    "\n",
    "The auc score for JC index:  0.59311582\n",
    "\n",
    "The auc score for AA index:  0.59906734\n",
    "\n",
    "The auc score for CN index:  0.5984018799999999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha * train_loss + (1-alpha)*loss\n",
    "\n",
    "    \n",
    "#for e in range(20):\n",
    "#    train_loss = 0\n",
    "#    for i in range(len(left_ids)):\n",
    "#        u = left_ids[i]\n",
    "#        v = right_ids[i]\n",
    "#        em = u * v\n",
    "#        y_pred = link_model(em)\n",
    "#        y_pred = torch.squeeze(y_pred)\n",
    "#        optimizer_link.zero_grad() \n",
    "#        loss = criterion(y_pred, y_label_model[i])\n",
    "#        train_loss += loss.item()\n",
    "#        loss.backward()\n",
    "#        optimizer_link.step()\n",
    "#    print(train_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
