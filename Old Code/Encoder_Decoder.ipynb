{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder-Decoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gracexwho/drug-drug-interactions/blob/master/Encoder_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEVK6hKim8Cp",
        "colab_type": "code",
        "outputId": "48a93c12-61d8-4c05-9181-524b7d1029ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from random import choice\n",
        "import urllib.request  # the lib that handles the url stuff\n",
        "import time\n",
        "\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/gracexwho/drug-drug-interactions/master/ChCh-Miner_durgbank-chem-chem.tsv\"\n",
        "url_data = urllib.request.urlopen(url) \n",
        "\n",
        "G = nx.read_edgelist(url_data)\n",
        "\n",
        "print(G.number_of_nodes())\n",
        "print(G.number_of_edges())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1514\n",
            "48514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc7XLYnf85be",
        "colab_type": "code",
        "outputId": "32744b94-36bf-42b1-bda2-65fa354d48f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Get training set validation set\n",
        "\n",
        "num_train = 100\n",
        "num_val = 20\n",
        "\n",
        "graph_nodes = list(G.nodes())\n",
        "training_nodes = torch.zeros([1, num_train], dtype=torch.float)\n",
        "validation_nodes = torch.zeros([1, num_val], dtype=torch.float)\n",
        "node_dict = {}\n",
        "# encode Nodes as numbers\n",
        "\n",
        "index = 0\n",
        "for node in list(G.nodes()):\n",
        "  node_dict[index] = node\n",
        "  index += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(num_train):\n",
        "  training_nodes[0][i] = choice(range(len(node_dict)))\n",
        "  if i < num_val:\n",
        "    validation_nodes[0][i] = choice(range(len(node_dict)))\n",
        "   \n",
        "\n",
        "print(training_nodes.shape)\n",
        "print(validation_nodes.shape)\n",
        "\n",
        "\n",
        "def Jaccard(T):\n",
        "  JC = {}\n",
        "  listNodes = list(T.nodes)\n",
        "  for u in listNodes:\n",
        "    for v in listNodes:\n",
        "      common = len(list(nx.common_neighbors(T, u, v)))\n",
        "      union = T.degree[u] + T.degree[v] - common\n",
        "      if union == 0:\n",
        "        JC[(u,v)] = 0\n",
        "      else:\n",
        "        JC[(u,v)] = (common/union)\n",
        "    \n",
        "  return JC\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "similarity = Jaccard(G)\n",
        "\n",
        "print(\"--- %s minutes ---\" % ((time.time() - start_time)//60))\n",
        "\n",
        "# 5:51:99 for 500 nodes in training_nodes"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100])\n",
            "torch.Size([1, 20])\n",
            "--- 6.0 minutes ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwx2FI7asKpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now creating the mapping\n",
        "# map nodes -> R^d\n",
        "# decoder in training\n",
        "# lesson 7: Encoders-Decoders\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  # should return VECTORS for each node\n",
        "  def __init__(self, num_train):\n",
        "    super(Encoder, self).__init__()\n",
        "    #self.fc1 = nn.Linear(1, 256)\n",
        "    #self.fc2 = nn.Linear(256, 64)\n",
        "    #self.fc3 = nn.Linear(64, 2)\n",
        "    \n",
        "    #self.dropout = nn.Dropout(p=0.2)\n",
        "    # one layer, return embeds(node_ids) which is a long tensor\n",
        "    #learnrate might be too big\n",
        "    self.embed = nn.Embedding(num_train, 256)\n",
        "    \n",
        "    \n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.embed(x)\n",
        "    return x\n",
        "\n",
        "# embeds.weight = embeds.weight/np.sqrt(mbed_dim)\n",
        "  # Loss function can't be a float, it should be a tensor\n",
        "  # and also DON'T unwrap a tensor at any point, that gets rid of grad\n",
        "  # Keep it in tensor operators: maybe change node_dict into a tensor\n",
        "  \n",
        "def LossFunction(nodes, similarity):\n",
        "  # decoder represents ALREADY DECODED results\n",
        "  # similarity represents evaluated similarities\n",
        "  # decode is a tuple (u,v) of integers\n",
        "  u, v = nodes\n",
        "  a = model(torch.tensor([[u]]))\n",
        "  b = model(torch.tensor([[v]]))\n",
        "  \n",
        "  decoded = Decoder(a,b)\n",
        "  sim = torch.tensor(similarity[(node_dict[u.item()], node_dict[v.item()])])\n",
        "  loss = (decoded) - sim       ##\n",
        "  loss = torch.pow(loss, 2)\n",
        "  return loss\n",
        "  \n",
        "  \n",
        "def Decoder(v1, v2):\n",
        "  v1 = torch.squeeze(v1)\n",
        "  v2 = torch.squeeze(v2)\n",
        "  # calculate inner product\n",
        "  return torch.dot(v1, v2)\n",
        "        \n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uILJxWP48hXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train\n",
        "\n",
        "\n",
        "model = Encoder()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "epochs = 5\n",
        "\n",
        " # have to somehow get vectors representing each node\n",
        " # pass in a SINGLE NODE to encoder and get back a vector\n",
        " # Comet.ml\n",
        "  \n",
        "\n",
        "encoded = {}\n",
        "\n",
        "for e in range(epochs):\n",
        "  total_lf = 0\n",
        "  running_loss = 0\n",
        "  for u in training_nodes[0]:\n",
        "      #print(encoded[u])\n",
        "      \n",
        "    for v in training_nodes[0]:\n",
        "      optimizer.zero_grad()\n",
        "          \n",
        "      total_lf = LossFunction((u, v), similarity)\n",
        "      running_loss += total_lf\n",
        "      total_lf.backward()        #retain_graph=True\n",
        "      optimizer.step()\n",
        "      #print(total_lf)\n",
        "\n",
        "  print(f\"Training loss: {running_Loss/num_train}\")\n",
        "      \n",
        "     # loss with AUC\n",
        "      # Friday 10:30-11 Meeting\n",
        "      \n",
        "        \n",
        "\n",
        "print(model.parameters())\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
